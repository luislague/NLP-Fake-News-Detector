{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Training 3\n",
    "In this notebook we are exploring the impact of using word embeddings on training the previous notebooks models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q spacy gensim nltk\n",
    "!python -m spacy download es_core_news_md\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'helpers' from '/notebooks/helpers.py'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# import custom helper module\n",
    "import importlib\n",
    "import helpers\n",
    "importlib.reload(helpers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = helpers.load_dataset(\"training_data_clean.csv\", force_reload=True)\n",
    "\n",
    "# print message\n",
    "helpers.print_text(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "[9780] jade helm really martial law texas ranger relay see inside military train --> 0\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print message\n",
    "helpers.print_text(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading spaCy and Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [republicans, punish, georgia, governor, refus...\n",
       "1    [father, soldier, slay, niger, defend, preside...\n",
       "2    [south, dakotas, governor, veto, loosen, conce...\n",
       "3    [turkeys, erdogan, say, take, jerusalem, resol...\n",
       "4    [bill, maher, insult, trump, suppose, masculin...\n",
       "5    [dem, senator, switch, party, call, nfl, owner...\n",
       "6    [ryan, say, trump, play, constructive, role, h...\n",
       "7    [epa, chief, want, scientist, debate, climate,...\n",
       "8    [macron, rebuffs, assad, accusations, france, ...\n",
       "9     [factbox, trump, fill, top, job, administration]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the news titles\n",
    "X_train_tok = X_train.apply(word_tokenize)\n",
    "X_test_tok = X_test.apply(word_tokenize)\n",
    "\n",
    "X_train_tok.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tokenized sentences\n",
    "sentences = X_train_tok.tolist()\n",
    "\n",
    "# Train the Word2Vec model\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained model for future use\n",
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1009098   0.42369646  0.16905278  0.11438417  0.15395679 -1.0888929\n",
      "  0.04600015  1.2877456  -0.45858213 -0.00980897 -0.61340445 -0.4006611\n",
      "  0.35187316  0.3254912   0.05728637 -0.90026647  0.3217223  -0.5676425\n",
      "  0.08375329 -1.6426866   0.87598896  0.08074313 -0.16266617  0.02741987\n",
      " -0.56688917  0.13562736 -0.3372227  -0.18075691 -0.53378326 -0.02543711\n",
      "  0.21432543  1.1239637   0.5955622  -0.20186833  0.10383294  0.39326224\n",
      " -0.7218565  -0.68641555 -0.34389305 -0.72810245  0.50160515 -0.46925777\n",
      " -0.45580345 -0.37470594 -0.00776394 -0.7345219  -1.1103567  -0.03710944\n",
      "  0.78701025  0.23681286 -0.05692272 -0.06359635 -0.01440202 -0.37049296\n",
      " -0.18431452  0.1768427  -0.00879263  0.43672043 -0.12425543  0.4377274\n",
      "  0.36666018  0.3076133  -0.49570087 -0.4651558  -0.30933484  0.8210563\n",
      "  0.3556036  -0.05598972 -0.5506664   0.17317593 -0.27808043 -0.04978522\n",
      "  0.04907046 -0.6970299   0.81413674  0.40380546  0.4773325  -0.51699656\n",
      " -0.3426096   0.42831302 -0.27656826  0.556901   -0.66858876  1.307835\n",
      " -0.07119309 -0.23868428  0.30947518  0.3280618   1.0820105  -0.10396413\n",
      "  1.003869    0.07051899  0.1380101   0.17594896  1.2072427   0.05470917\n",
      " -0.15312353 -0.90936375  0.0876142   0.1758372 ]\n",
      "[('abc', 0.97138512134552), ('fox', 0.9683234691619873), ('cnn', 0.9677236080169678), ('daily', 0.9642089605331421), ('bad', 0.9598856568336487), ('devastating', 0.958344578742981), ('segment', 0.9524346590042114), ('network', 0.9509928822517395), ('anchor', 0.9491063952445984), ('nbc', 0.9468369483947754)]\n"
     ]
    }
   ],
   "source": [
    "# Get the vector for the word 'fake'\n",
    "vector = w2v_model.wv['fake']\n",
    "print(vector)\n",
    "\n",
    "# Find most similar words to 'fake'\n",
    "similar_words = w2v_model.wv.most_similar('fake')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average word2vec for each sentence\n",
    "def sentence_to_vec(sentence, model):\n",
    "    # Filter out words that are not in the Word2Vec vocabulary\n",
    "    words_in_vocab = [word for word in sentence if word in model.wv]\n",
    "    \n",
    "    # If none of the words in the sentence are in the vocabulary, return a zero vector\n",
    "    if len(words_in_vocab) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Average the word vectors for all words in the sentence\n",
    "    return np.mean([model.wv[word] for word in words_in_vocab], axis=0)\n",
    "\n",
    "# Apply to your dataset to get sentence embeddings for all titles\n",
    "X_train_emb = X_train_tok.apply(lambda x: sentence_to_vec(x, w2v_model))\n",
    "X_test_emb = X_test_tok.apply(lambda x: sentence_to_vec(x, w2v_model))\n",
    "\n",
    "# Convert the list of embeddings to a NumPy array for model training\n",
    "X_train_emb = np.array(X_train_emb.tolist())\n",
    "X_test_emb = np.array(X_test_emb.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X contains your sentence embeddings (e.g., from Word2Vec)\n",
    "# Normalize the embeddings using L2 norm\n",
    "X_train_norm = normalize(X_train_emb, norm='l2')\n",
    "X_test_norm = normalize(X_test_emb, norm='l2')\n",
    "\n",
    "# Now X_normalized contains L2-normalized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86      3515\n",
      "           1       0.86      0.84      0.85      3316\n",
      "\n",
      "    accuracy                           0.85      6831\n",
      "   macro avg       0.85      0.85      0.85      6831\n",
      "weighted avg       0.85      0.85      0.85      6831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_norm, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_norm)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.8647342995169082\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87      3515\n",
      "           1       0.87      0.85      0.86      3316\n",
      "\n",
      "    accuracy                           0.86      6831\n",
      "   macro avg       0.86      0.86      0.86      6831\n",
      "weighted avg       0.86      0.86      0.86      6831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM model\n",
    "svm_model = SVC(kernel='linear', C=1.0, random_state=42)  # You can experiment with the 'kernel' parameter\n",
    "svm_model.fit(X_train_norm, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test_norm)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM Accuracy: {accuracy}\")\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      3515\n",
      "           1       0.89      0.88      0.88      3316\n",
      "\n",
      "    accuracy                           0.89      6831\n",
      "   macro avg       0.89      0.89      0.89      6831\n",
      "weighted avg       0.89      0.89      0.89      6831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataset into DMatrix, XGBoost's internal data structure\n",
    "train_dmatrix = xgb.DMatrix(X_train_norm, label=y_train)\n",
    "test_dmatrix = xgb.DMatrix(X_test_norm, label=y_test)\n",
    "\n",
    "# Set up the XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Binary classification\n",
    "    'eval_metric': 'logloss',        # Logarithmic loss as evaluation metric\n",
    "    'max_depth': 6,                  # Maximum depth of a tree\n",
    "    'eta': 0.1,                      # Learning rate\n",
    "    'subsample': 0.8,                # Fraction of samples used per tree\n",
    "    'colsample_bytree': 0.8,         # Fraction of features used per tree\n",
    "    'seed': 42                       # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = xgb.train(params, train_dmatrix, num_boost_round=100)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = model.predict(test_dmatrix)\n",
    "y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_prob]  # Convert probabilities to binary predictions\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200, 'subsample': 1.0}\n",
      "XGBoost (with tuning) Accuracy: 0.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      3515\n",
      "           1       0.89      0.88      0.88      3316\n",
      "\n",
      "    accuracy                           0.89      6831\n",
      "   macro avg       0.89      0.89      0.89      6831\n",
      "weighted avg       0.89      0.89      0.89      6831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize XGBClassifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 6],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 200],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearch\n",
    "grid_search.fit(X_train_norm, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "y_pred = grid_search.best_estimator_.predict(X_test_norm)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost (with tuning) Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 89.28%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90      3515\n",
      "           1       0.90      0.88      0.89      3316\n",
      "\n",
      "    accuracy                           0.89      6831\n",
      "   macro avg       0.89      0.89      0.89      6831\n",
      "weighted avg       0.89      0.89      0.89      6831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try Random Forest Classifier\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train_norm, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_rf = rf_classifier.predict(X_test_norm)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
